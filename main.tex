%
% File naaclhlt2016.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2016}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{subcaption}

% \naaclfinalcopy % Uncomment this line for the final submission
\def\naaclpaperid{***} %  Enter the naacl Paper ID here

% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}    

\newcommand\BibTeX{B{\sc ib}\TeX}


\title{Crowd Knows Better: Crowdsourcing for Non-factoid Question Answering}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
% If the title and author information does not fit in the area allocated,
% place \setlength\titlebox{<new height>} right after
% at the top, where <new height> can be something larger than 2.25in

\author{Denis Savenkov \\ Emory University \\ {\tt dsavenk@emory.edu} 
  \And Scott Weitzner \\ Emory University \\ {\tt sweitzn@emory.edu}
  \And Eugene Agichtein \\ Emory University \\ {\tt eugene@mathcs.emory.edu}
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
State the problem.
Why is it an interesting problem.
What is our contribution.
What are implications and value.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Describe the problem.
State contributions.

We focus on the following research questions:
\begin{enumerate}
\item RQ1. Can crowdsourcing be used to judge the quality of answers to non-factoid questions under time limit?
\item RQ2. Is it possible to use crowdsourcing to collect answers to real user questions under time limit?
\item RQ3. How the quality of crowdsourced answers to non-factoid question compare to original CQA answers and automatic answers from TREC LiveQA systems?
\end{enumerate}


\section{Methodology}
\label{sec:methodology}

\subsection{Crowdsourcing Interface}
\label{subsec:interface}



This section will describe the setup of our crowdsourcing experiments.




\section{Results}
\label{sec:results}

First, we will describe results on crowdsourcing answer ratings, because we are planning to use these scores to estimate the quality of generated answers.

\subsection{Answer validation}

To get ratings for answers and compare their quality against trained NIST assessors, we sampled 200 questions from TREC LiveQA 2015, and took answers from 3 different systems.
The interest of this work is in real-time scenario, therefore we made two experiments: with and without time pressure.
In the first experiment workers were given a minute to provide their judgments, and in the second experiment the working time wasn't limited.
Figure \ref{figure:score_correlation} shows correlation between official NIST assessor relevance judgments and ratings provided by our workers.
Results showed, that Pearson correlation between official scores and our ratings is $\rho=0.52$.
The distribution of scores shows, that official assessors were very generous in giving the lowest score of 1, mechanical turk scores tend to be a little higher and they assign extreme score less often.

However, in general, the results suggest that we can trust crowdsourced ratings and we will use them in our next experiment to estimate the quality of different answers.

\begin{figure}[t!]
	\centering
	\includegraphics[width=0.5\textwidth]{img/score_correlation}
	\caption{Correlation between NIST assessor scores and crowdsourced ratings with and without time limit on the work time}
	\label{figure:score_correlation}
\end{figure}
	
Figure \ref{figure:validation_time} shows that even though the median time to rate all three answers is around 22-25 seconds in both experiments, the upper bound is significantly lower in the experiment with the time pressure.
	
\begin{figure}[t!]
	\centering
	\includegraphics[width=0.5\textwidth]{img/validation_time}
	\caption{Box plot of rating time with and without time pressure}
	\label{figure:validation_time}
\end{figure}

\subsection{Answer crowdsourcing}

\begin{table}[h]
\centering
\caption{Statistics of different types of answers for Yahoo! Answers questions}
\begin{tabular}{| p{2.3cm} | c | c | c |}
\hline
Statistic & Y!A & mTurk & mTurk-time\\
\hline
\% answered & 78.7\% & 100.0\% & 100.0\% \\
Length (chars) & 354.96 & 190.83 & 126.65 \\
Length (words) & 64.54 & 34.16 & 22.82 \\
\hline
\end{tabular}
\label{table:answer_stats}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{img/average_score}
\caption{Average scores of different type of answers to Yahoo! Answers questions for different categories}
\label{fig:average_score}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{img/scores_distribution}
\caption{Distribution of scores for different types of answers to Yahoo! Answers questions}
\label{fig:scores_distribution}
\end{figure}

\section{Discussion}
\label{sec:discussion}

Some discussion on the results and possible future work.

\section{Related Work}
\label{sec:related_work}

In \cite{bernstein2012direct} authors used a combination of log mining and crowdsourcing to build answers to a long tail of questions, that search engine users ask, therefore increasing the coverage of direct answers

\section{Conclusion}
\label{sec:conclusion}

Final words.

\section*{Acknowledgments}

If any...

\bibliography{naaclhlt2016}
\bibliographystyle{naaclhlt2016}


\end{document}
